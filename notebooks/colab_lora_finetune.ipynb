{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab LoRA Fine-Tuning Tool\n",
        "\n",
        "This notebook fine-tunes open-source chat models (e.g., `meta-llama/Llama-2-7b-chat-hf`, `mistralai/Mistral-7B-Instruct`) using standard LoRA adapters without 4-bit quantization. It targets users who have at least a single 16–24 GB GPU available in Google Colab and prefer a simpler, full-precision setup compared to QLoRA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Runtime checklist\n",
        "\n",
        "1. Go to **Runtime ▸ Change runtime type** and choose `T4` (free) or `L4/A100` (Colab Pro/Pro+).\n",
        "2. Ensure the **Hardware accelerator** is set to GPU.\n",
        "3. After connecting, run the cell below to confirm that the GPU is visible to PyTorch.\n",
        "\n",
        "> Full-precision LoRA consumes more VRAM than QLoRA. If you only have ~16 GB, prefer 7B models with modest batch sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U accelerate==0.30.1 datasets==2.19.1 evaluate==0.4.2 huggingface_hub==0.24.5 peft==0.11.1 sentencepiece==0.1.99 transformers==4.44.2 trl==0.9.4 wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=input(\"Paste your Hugging Face access token: \").strip(), add_to_git_credential=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data ingestion options\n",
        "\n",
        "- **Upload `.txt`/`.json` files** to Colab and point `cfg.text_folder` to `/content/data`.\n",
        "- **Mount Google Drive** for larger corpora:\n",
        "  ```python\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  ```\n",
        "- **Reference a Hugging Face dataset** (e.g., `tatsu-lab/alpaca`) by setting `cfg.dataset_source=\"hf_dataset\"`.\n",
        "\n",
        "The helper below normalizes data into `instruction`, `response`, and optional `system` fields before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    project_name: str = \"lora-full-precision\"\n",
        "    base_model: str = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "    dataset_source: str = \"text_folder\"  # text_folder | hf_dataset\n",
        "    text_folder: str = \"/content/data\"\n",
        "    hf_dataset: str | None = None\n",
        "    max_samples: int | None = None\n",
        "    chunk_tokens: int = 1024\n",
        "    chunk_overlap: int = 128\n",
        "    system_prompt: str = \"You are a helpful assistant.\"\n",
        "    output_dir: str = \"/content/lora-output\"\n",
        "    wandb_project: str | None = None\n",
        "\n",
        "    micro_batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 8\n",
        "    epochs: float = 3.0\n",
        "    learning_rate: float = 1e-4\n",
        "    warmup_ratio: float = 0.03\n",
        "    weight_decay: float = 0.0\n",
        "    cutoff_len: int = 2048\n",
        "    lora_r: int = 16\n",
        "    lora_alpha: int = 32\n",
        "    lora_dropout: float = 0.05\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "print(asdict(cfg))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "\n",
        "def _normalize(text: str) -> str:\n",
        "    text = text.replace(\"\\r\", \" \").strip()\n",
        "    return re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "\n",
        "def _chunk_words(text: str, chunk_tokens: int, overlap: int) -> list[str]:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    step = max(chunk_tokens - overlap, 1)\n",
        "    chunks = []\n",
        "    for start in range(0, len(words), step):\n",
        "        segment = words[start:start + chunk_tokens]\n",
        "        if len(segment) < 32:\n",
        "            continue\n",
        "        chunks.append(\" \".join(segment))\n",
        "    return chunks or [\" \".join(words[:chunk_tokens])]\n",
        "\n",
        "\n",
        "def _load_local_texts(folder: str, cfg: Config) -> list[dict]:\n",
        "    folder_path = Path(folder)\n",
        "    rows = []\n",
        "    for path in folder_path.rglob(\"*.txt\"):\n",
        "        text = path.read_text(encoding=\"utf-8\")\n",
        "        for chunk in _chunk_words(_normalize(text), cfg.chunk_tokens, cfg.chunk_overlap):\n",
        "            rows.append({\n",
        "                \"instruction\": f\"Answer using {path.stem} context.\",\n",
        "                \"response\": chunk,\n",
        "                \"system\": cfg.system_prompt,\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def _load_hf_dataset(repo_id: str, cfg: Config) -> list[dict]:\n",
        "    ds = load_dataset(repo_id, split=\"train\")\n",
        "    rows = []\n",
        "    for row in ds:\n",
        "        if {\"instruction\", \"output\"}.issubset(ds.column_names):\n",
        "            instruction = row[\"instruction\"]\n",
        "            response = row[\"output\"]\n",
        "        else:\n",
        "            instruction = row.get(\"instruction\", \"Summarize the passage:\")\n",
        "            response = row.get(\"text\", row.get(\"response\", \"\"))\n",
        "        rows.append({\n",
        "            \"instruction\": _normalize(str(instruction)),\n",
        "            \"response\": _normalize(str(response)),\n",
        "            \"system\": row.get(\"system\", cfg.system_prompt),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def build_dataset(cfg: Config) -> Dataset:\n",
        "    if cfg.dataset_source == \"hf_dataset\" and cfg.hf_dataset:\n",
        "        rows = _load_hf_dataset(cfg.hf_dataset, cfg)\n",
        "    else:\n",
        "        rows = _load_local_texts(cfg.text_folder, cfg)\n",
        "\n",
        "    if cfg.max_samples:\n",
        "        random.seed(cfg.seed)\n",
        "        rows = random.sample(rows, min(cfg.max_samples, len(rows)))\n",
        "\n",
        "    rows = [r for r in rows if r[\"instruction\"].strip() and r[\"response\"].strip()]\n",
        "    dataset = Dataset.from_pandas(pd.DataFrame(rows))\n",
        "    print(f\"Dataset has {len(dataset)} rows after cleaning\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = build_dataset(cfg)\n",
        "dataset[:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt templating\n",
        "\n",
        "We build chat prompts using the tokenizer's `chat_template` when available so that LoRA adapters stay aligned with the base instruction format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=False)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def build_prompt(example: dict) -> dict:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": example.get(\"system\") or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
        "    ]\n",
        "    if tokenizer.chat_template:\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"[SYSTEM]\\n{messages[0]['content']}\\n\\n\"\n",
        "            f\"[USER]\\n{messages[1]['content']}\\n\\n\"\n",
        "            f\"[ASSISTANT]\\n{messages[2]['content']}\"\n",
        "        )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "\n",
        "processed_dataset = dataset.map(build_prompt, remove_columns=dataset.column_names)\n",
        "processed_dataset = processed_dataset.shuffle(seed=cfg.seed)\n",
        "splits = processed_dataset.train_test_split(test_size=0.05, seed=cfg.seed)\n",
        "splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LoRA training\n",
        "\n",
        "Without quantization, we load the base model in bf16/fp16, attach LoRA adapters to attention/MLP modules, and fine-tune via `trl.SFTTrainer`. Keep `micro_batch_size` low to stay within VRAM limits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "if cfg.wandb_project:\n",
        "    os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project\n",
        "else:\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.base_model,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=cfg.lora_r,\n",
        "    lora_alpha=cfg.lora_alpha,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=cfg.lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=cfg.output_dir,\n",
        "    per_device_train_batch_size=cfg.micro_batch_size,\n",
        "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
        "    num_train_epochs=cfg.epochs,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    weight_decay=cfg.weight_decay,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    bf16=torch_dtype == torch.bfloat16,\n",
        "    fp16=torch_dtype == torch.float16,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=([] if os.environ.get(\"WANDB_DISABLED\") == \"true\" else [\"wandb\"]),\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=splits[\"train\"],\n",
        "    eval_dataset=splits[\"test\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=cfg.cutoff_len,\n",
        "    packing=True,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(cfg.output_dir)\n",
        "tokenizer.save_pretrained(cfg.output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def compute_perplexity(eval_dataset, max_batches: int = 32) -> float:\n",
        "    model.eval()\n",
        "    loader = DataLoader(eval_dataset[\"text\"], batch_size=1)\n",
        "    scores = []\n",
        "    for idx, batch in enumerate(loader):\n",
        "        if idx >= max_batches:\n",
        "            break\n",
        "        encoded = tokenizer(batch[0], return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded, labels=encoded[\"input_ids\"])\n",
        "        scores.append(math.exp(outputs.loss.item()))\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "perplexity = compute_perplexity(splits[\"test\"], max_batches=32)\n",
        "print(f\"Approximate perplexity: {perplexity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(prompt: str, system: str | None = None, max_new_tokens: int = 512) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(template, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    output_tokens = generated[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "chat(\"Summarize the three biggest takeaways from our training data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "MERGED_DIR = Path(cfg.output_dir) / \"merged\"\n",
        "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "merged_model = AutoPeftModelForCausalLM.from_pretrained(cfg.output_dir, device_map=\"auto\")\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "merged_model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "print(\"Merged checkpoint saved to\", MERGED_DIR)\n",
        "\n",
        "push_to_hub = False\n",
        "if push_to_hub:\n",
        "    repo_id = input(\"Target HF repo (username/project): \")\n",
        "    merged_model.push_to_hub(repo_id, private=True)\n",
        "    tokenizer.push_to_hub(repo_id, private=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Adjust the config block (`cfg`) for your dataset, model, and hyperparameters before running.\n",
        "- Monitor GPU memory via `nvidia-smi` while training; reduce `micro_batch_size` or `cutoff_len` if you hit OOM.\n",
        "- Evaluate the resulting adapter on task-specific prompts and log findings (W&B, TensorBoard, etc.).\n",
        "- Deploy adapters in production by loading them with `PeftModel.from_pretrained` inside your inference stack (vLLM, TGI, etc.), or use the merged checkpoint saved above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
