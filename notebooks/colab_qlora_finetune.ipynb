{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab QLoRA Fine-Tuning Tool\n",
        "\n",
        "This notebook walks you through training instruction-following adapters (LoRA) on top of open-source chat models such as `meta-llama/Llama-2-7b-chat-hf` using the QLoRA technique. Each section explains **what** to run and **why it matters** so you can confidently adapt the workflow to new datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š What is QLoRA? (Quantized Low-Rank Adaptation)\n",
        "\n",
        "**QLoRA** was introduced in the paper *\"QLoRA: Efficient Finetuning of Quantized LLMs\"* (Dettmers et al., 2023). It combines two powerful techniques:\n",
        "\n",
        "1. **LoRA** (Low-Rank Adaptation) - Train small adapter matrices instead of the full model\n",
        "2. **4-bit Quantization** - Compress the base model to use 75% less memory\n",
        "\n",
        "This allows fine-tuning a **65B parameter model on a single 48GB GPU** or a **7B model on a free Google Colab T4 (16GB)**!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§  Understanding Quantization\n",
        "\n",
        "### What is Quantization?\n",
        "\n",
        "Quantization reduces the precision of model weights from 32-bit or 16-bit floats to lower bit representations.\n",
        "\n",
        "```\n",
        "Original (FP16):     [0.00012, -0.00234, 0.00567, ...]  â†’ 16 bits per value\n",
        "Quantized (4-bit):   [2, -5, 11, ...]                   â†’ 4 bits per value\n",
        "                                                         (75% memory savings!)\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Types of Quantization\n",
        "\n",
        "| Type | Bits | Memory (7B model) | Quality |\n",
        "|------|------|-------------------|---------|\n",
        "| FP32 | 32 | ~28 GB | Best |\n",
        "| FP16/BF16 | 16 | ~14 GB | Excellent |\n",
        "| INT8 | 8 | ~7 GB | Very Good |\n",
        "| **NF4 (QLoRA)** | **4** | **~3.5 GB** | **Good** |\n",
        "\n",
        "### ğŸŒŸ NF4: Normal Float 4-bit\n",
        "\n",
        "QLoRA uses a special 4-bit format called **NF4 (Normal Float 4-bit)**:\n",
        "\n",
        "> NF4 is information-theoretically optimal for normally distributed weights.\n",
        "\n",
        "Neural network weights typically follow a normal (Gaussian) distribution centered around zero. NF4 places quantization levels optimally for this distribution:\n",
        "\n",
        "```\n",
        "Standard INT4:     Uniform spacing: -8, -7, -6, ... 0, ... 6, 7\n",
        "NF4:               Optimal spacing:  More levels near 0 (where most weights are)\n",
        "                   \n",
        "                   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
        "                â–ˆâ–ˆâ–ˆâ–ˆ                        â–ˆâ–ˆâ–ˆâ–ˆ\n",
        "              â–ˆâ–ˆ                                â–ˆâ–ˆ\n",
        "            â–ˆâ–ˆ                                    â–ˆâ–ˆ\n",
        "          â–ˆâ–ˆ                                        â–ˆâ–ˆ\n",
        "        â–ˆâ–ˆ                                            â–ˆâ–ˆ\n",
        "      â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€\n",
        "        More quantization levels near 0 (peak of distribution)\n",
        "```\n",
        "\n",
        "### ğŸ”„ Double Quantization\n",
        "\n",
        "QLoRA also uses **double quantization** to further reduce memory:\n",
        "\n",
        "1. **First quantization**: Compress weights to 4-bit with scaling factors\n",
        "2. **Second quantization**: Compress the scaling factors themselves to 8-bit\n",
        "\n",
        "This saves an additional ~0.4 bits per parameter!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”§ How QLoRA Works\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                         QLoRA ARCHITECTURE                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                    â”‚\n",
        "â”‚  Base Model (Frozen, 4-bit quantized)                             â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚  â”‚  Attention Layer                                             â”‚  â”‚\n",
        "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚\n",
        "â”‚  â”‚  â”‚ q_proj (4-bit)  â”‚ â†â”€â”€ â”‚ LoRA A,B (fp16) â”‚ â† TRAINABLE    â”‚  â”‚\n",
        "â”‚  â”‚  â”‚ k_proj (4-bit)  â”‚ â†â”€â”€ â”‚ LoRA A,B (fp16) â”‚ â† TRAINABLE    â”‚  â”‚\n",
        "â”‚  â”‚  â”‚ v_proj (4-bit)  â”‚ â†â”€â”€ â”‚ LoRA A,B (fp16) â”‚ â† TRAINABLE    â”‚  â”‚\n",
        "â”‚  â”‚  â”‚ o_proj (4-bit)  â”‚ â†â”€â”€ â”‚ LoRA A,B (fp16) â”‚ â† TRAINABLE    â”‚  â”‚\n",
        "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚\n",
        "â”‚  â”‚                                                              â”‚  â”‚\n",
        "â”‚  â”‚  MLP Layer                                                   â”‚  â”‚\n",
        "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚\n",
        "â”‚  â”‚  â”‚ gate_proj (4-bit)â”‚ â†â”€â”€ â”‚ LoRA A,B (fp16) â”‚ â† TRAINABLE   â”‚  â”‚\n",
        "â”‚  â”‚  â”‚ up_proj (4-bit)  â”‚ â†â”€â”€ â”‚ LoRA A,B (fp16) â”‚ â† TRAINABLE   â”‚  â”‚\n",
        "â”‚  â”‚  â”‚ down_proj (4-bit)â”‚ â†â”€â”€ â”‚ LoRA A,B (fp16) â”‚ â† TRAINABLE   â”‚  â”‚\n",
        "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                                                                    â”‚\n",
        "â”‚  Forward Pass:                                                     â”‚\n",
        "â”‚  1. Dequantize 4-bit weights â†’ BF16 on-the-fly                    â”‚\n",
        "â”‚  2. Compute: output = W_dequant(x) + B(A(x)) Ã— (Î±/r)              â”‚\n",
        "â”‚  3. Only LoRA gradients computed (base model frozen)              â”‚\n",
        "â”‚                                                                    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ§® The Mathematics\n",
        "\n",
        "For each adapted layer, the forward pass is:\n",
        "\n",
        "$$y = W_{4bit}^{dequant} \\cdot x + \\frac{\\alpha}{r} \\cdot B \\cdot A \\cdot x$$\n",
        "\n",
        "Where:\n",
        "- $W_{4bit}^{dequant}$: Base weights dequantized from 4-bit to bf16 on-the-fly\n",
        "- $A \\in \\mathbb{R}^{r \\times k}$: LoRA down-projection (trainable, fp16)\n",
        "- $B \\in \\mathbb{R}^{d \\times r}$: LoRA up-projection (trainable, fp16)\n",
        "- $\\alpha$: Scaling factor (`lora_alpha`)\n",
        "- $r$: LoRA rank\n",
        "\n",
        "### ğŸ“Š Memory Comparison\n",
        "\n",
        "| Setup | Model Memory | Trainable Params | Total VRAM (7B) |\n",
        "|-------|--------------|------------------|-----------------|\n",
        "| Full Fine-tuning (fp16) | ~14 GB | 7B | ~56 GB |\n",
        "| LoRA (fp16) | ~14 GB | ~20M | ~15 GB |\n",
        "| **QLoRA (4-bit + LoRA)** | **~3.5 GB** | **~20M** | **~6 GB** |\n",
        "\n",
        "### âœ… Advantages of QLoRA\n",
        "\n",
        "1. **Extreme memory efficiency**: Fine-tune 7B models on 8GB GPUs\n",
        "2. **Near full-precision quality**: Only ~0.1% quality loss vs fp16 fine-tuning\n",
        "3. **Fast inference**: 4-bit models run faster with less memory bandwidth\n",
        "4. **Enables larger models**: Fine-tune 13B, 30B, 65B models on consumer hardware\n",
        "\n",
        "### âš ï¸ Trade-offs\n",
        "\n",
        "| Aspect | QLoRA | Standard LoRA |\n",
        "|--------|-------|---------------|\n",
        "| Memory | Very Low | Moderate |\n",
        "| Training Speed | Slightly Slower (dequantization) | Faster |\n",
        "| Quality | ~99% of LoRA | Reference |\n",
        "| Setup Complexity | Requires bitsandbytes | Simpler |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ–¥ï¸ Runtime Checklist\n",
        "\n",
        "1. Go to **Runtime â–¸ Change runtime type** and pick `T4 GPU` (L4/A100 if you have Colab Pro/Pro+).\n",
        "2. Toggle **GPU** and keep the rest default.\n",
        "3. Connect the runtime before running the cells below.\n",
        "\n",
        "> **Why:** QLoRA loads the base model in 4-bit precision, so a T4's 16 GB VRAM is sufficient for 7B chat models when you keep batch sizes modest.\n",
        "\n",
        "### ğŸ’¾ Memory Breakdown for QLoRA\n",
        "\n",
        "| Component | Memory (7B model) |\n",
        "|-----------|-------------------|\n",
        "| 4-bit quantized weights | ~3.5 GB |\n",
        "| LoRA adapters (fp16) | ~50-100 MB |\n",
        "| Activations & gradients | ~2-4 GB |\n",
        "| Optimizer states (LoRA only) | ~200 MB |\n",
        "| **Total** | **~6-8 GB** âœ… Fits on T4! |\n",
        "\n",
        "Compare this to full fine-tuning which would need ~56 GB!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“¦ Installing Dependencies\n",
        "\n",
        "Key libraries for QLoRA:\n",
        "- **`bitsandbytes`**: Implements 4-bit quantization (NF4) and 8-bit optimizers\n",
        "- **`peft`**: Parameter-Efficient Fine-Tuning library (LoRA implementation)\n",
        "- **`transformers`**: Hugging Face model loading with quantization support\n",
        "- **`trl`**: Provides `SFTTrainer` for supervised fine-tuning\n",
        "- **`accelerate`**: Handles device placement and mixed precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%capture\n",
        "%pip install -U accelerate==0.27.2 bitsandbytes==0.43.0 datasets==2.17.0 evaluate==0.4.1 huggingface_hub==0.21.4 peft==0.8.2 sentencepiece==0.1.99 transformers==4.38.2 trl==0.7.10 wandb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=input(\"Paste your Hugging Face access token: \").strip(), add_to_git_credential=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“‚ Data Ingestion Options\n",
        "\n",
        "- **Upload text files** (left sidebar â–¸ Files â–¸ Upload) and set `DATASET_SOURCE=\"text_folder\"`.\n",
        "- **Mount Google Drive** for larger corpora:\n",
        "  ```python\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  ```\n",
        "- **Use a public Hugging Face dataset** by supplying its repo id (e.g., `tatsu-lab/alpaca`).\n",
        "\n",
        "The helper below consolidates these flows and creates a `datasets.Dataset` with `instruction`, `response`, and optional `system` fields.\n",
        "\n",
        "### ğŸ“ Understanding Instruction-Tuning Data\n",
        "\n",
        "Instruction-tuning teaches the model to follow human instructions. The data format:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"Explain quantum computing in simple terms\",\n",
        "  \"response\": \"Quantum computing uses quantum mechanics principles...\",\n",
        "  \"system\": \"You are a helpful science educator.\"\n",
        "}\n",
        "```\n",
        "\n",
        "### ğŸ“Š Config Class: QLoRA-Specific Settings\n",
        "\n",
        "The configuration below includes parameters optimized for QLoRA training:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    project_name: str = \"qlora-custom-data\"\n",
        "    base_model: str = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "    dataset_source: str = \"text_folder\"  # text_folder | hf_dataset\n",
        "    text_folder: str = \"/content/data\"   # ignored when using hf_dataset\n",
        "    hf_dataset: str | None = None         # e.g. \"tatsu-lab/alpaca\"\n",
        "    max_samples: int | None = None\n",
        "    chunk_tokens: int = 1024\n",
        "    chunk_overlap: int = 128\n",
        "    system_prompt: str = \"You are a helpful assistant that responds with concise, domain-specific answers.\"\n",
        "    output_dir: str = \"/content/qlora-output\"\n",
        "    wandb_project: str | None = None\n",
        "\n",
        "    # Training hyperparams\n",
        "    micro_batch_size: int = 4\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    epochs: float = 3.0\n",
        "    learning_rate: float = 2e-4\n",
        "    warmup_ratio: float = 0.03\n",
        "    weight_decay: float = 0.0\n",
        "    cutoff_len: int = 2048\n",
        "    lora_r: int = 64\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.1\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "print(asdict(cfg))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ” QLoRA Config Parameters Explained\n",
        "\n",
        "**LoRA Hyperparameters (QLoRA uses higher rank due to quantization):**\n",
        "- `lora_r=64`: Higher rank compensates for quantization information loss\n",
        "- `lora_alpha=16`: Scaling factor; effective scaling is `16/64 = 0.25`\n",
        "- `lora_dropout=0.1`: Slightly higher dropout for regularization\n",
        "\n",
        "**Training Hyperparameters:**\n",
        "- `micro_batch_size=4`: QLoRA uses less memory, so we can use larger batches!\n",
        "- `learning_rate=2e-4`: Slightly higher LR works well with QLoRA\n",
        "- `gradient_accumulation_steps=4`: Effective batch = 4 Ã— 4 = 16\n",
        "\n",
        "**Why higher `lora_r` for QLoRA?**\n",
        "The 4-bit quantization loses some expressiveness compared to fp16. A higher LoRA rank (64 vs 16) compensates by giving the adapter more capacity to learn task-specific adjustments.\n",
        "\n",
        "### ğŸ“ˆ Data Processing Pipeline\n",
        "\n",
        "The following code handles:\n",
        "1. **Text normalization**: Standardize whitespace and encoding\n",
        "2. **Chunking with overlap**: Split long documents while preserving context\n",
        "3. **Format unification**: Convert various formats to instruction/response pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "\n",
        "def _normalize(text: str) -> str:\n",
        "    text = text.replace(\"\\r\", \" \").strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def _chunk_words(text: str, chunk_tokens: int, overlap: int) -> list[str]:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    step = max(chunk_tokens - overlap, 1)\n",
        "    chunks = []\n",
        "    for start in range(0, len(words), step):\n",
        "        segment = words[start:start + chunk_tokens]\n",
        "        if len(segment) < 32:\n",
        "            continue\n",
        "        chunks.append(\" \".join(segment))\n",
        "    return chunks or [\" \".join(words[:chunk_tokens])]\n",
        "\n",
        "\n",
        "def _load_local_texts(folder: str, cfg: Config) -> list[dict]:\n",
        "    folder_path = Path(folder)\n",
        "    rows = []\n",
        "    for path in folder_path.rglob(\"*.txt\"):\n",
        "        text = path.read_text(encoding=\"utf-8\")\n",
        "        for chunk in _chunk_words(_normalize(text), cfg.chunk_tokens, cfg.chunk_overlap):\n",
        "            rows.append({\n",
        "                \"instruction\": f\"Answer the following based on {path.stem}:\",\n",
        "                \"response\": chunk,\n",
        "                \"system\": cfg.system_prompt,\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def _load_hf_dataset(repo_id: str, cfg: Config) -> list[dict]:\n",
        "    ds = load_dataset(repo_id, split=\"train\")\n",
        "    fields = set(ds.column_names)\n",
        "    rows = []\n",
        "    for row in ds:\n",
        "        if {\"instruction\", \"output\"}.issubset(fields):\n",
        "            response = row[\"output\"]\n",
        "            instruction = row[\"instruction\"]\n",
        "        else:\n",
        "            # Fallback to single text field\n",
        "            response = row.get(\"text\", row.get(\"response\", \"\"))\n",
        "            instruction = row.get(\"instruction\", \"Summarize the passage:\")\n",
        "        rows.append({\n",
        "            \"instruction\": _normalize(str(instruction)),\n",
        "            \"response\": _normalize(str(response)),\n",
        "            \"system\": row.get(\"system\", cfg.system_prompt),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def build_dataset(cfg: Config) -> Dataset:\n",
        "    if cfg.dataset_source == \"hf_dataset\" and cfg.hf_dataset:\n",
        "        rows = _load_hf_dataset(cfg.hf_dataset, cfg)\n",
        "    else:\n",
        "        rows = _load_local_texts(cfg.text_folder, cfg)\n",
        "\n",
        "    if cfg.max_samples:\n",
        "        random.seed(cfg.seed)\n",
        "        rows = random.sample(rows, min(cfg.max_samples, len(rows)))\n",
        "\n",
        "    clean_rows = [r for r in rows if r[\"instruction\"].strip() and r[\"response\"].strip()]\n",
        "    dataset = Dataset.from_pandas(pd.DataFrame(clean_rows))\n",
        "    print(f\"Dataset has {len(dataset)} rows after cleaning\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = build_dataset(cfg)\n",
        "dataset[:2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ’¬ Prompt Templating\n",
        "\n",
        "We rely on the tokenizer's built-in chat template (when available) so that formatting always matches the base model's expectations. If the checkpoint lacks a template, we fall back to a simple system/instruction/response triple.\n",
        "\n",
        "### ğŸ¯ Why Templates Matter for Fine-Tuning\n",
        "\n",
        "The pre-trained model learned specific patterns during its original training. Using the correct template ensures:\n",
        "1. **Consistency**: Same format during training and inference\n",
        "2. **Better learning**: Model recognizes the familiar structure\n",
        "3. **Proper generation**: Stop tokens and role markers work correctly\n",
        "\n",
        "### ğŸ“ Example Llama-2 Chat Template\n",
        "\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "You are a helpful assistant that responds with concise, domain-specific answers.\n",
        "<</SYS>>\n",
        "\n",
        "Answer the following based on document_1: [/INST] Here is the relevant information from document_1... </s>\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=False)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def build_prompt(example: dict) -> dict:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": example.get(\"system\") or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
        "    ]\n",
        "    if tokenizer.chat_template:\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"[SYSTEM]\\n{messages[0]['content']}\\n\\n\"\n",
        "            f\"[USER]\\n{messages[1]['content']}\\n\\n\"\n",
        "            f\"[ASSISTANT]\\n{messages[2]['content']}\"\n",
        "        )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "\n",
        "processed_dataset = dataset.map(build_prompt, remove_columns=dataset.column_names)\n",
        "processed_dataset = processed_dataset.shuffle(seed=cfg.seed)\n",
        "splits = processed_dataset.train_test_split(test_size=0.05, seed=cfg.seed)\n",
        "splits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸš€ QLoRA Training\n",
        "\n",
        "We load the base model in 4-bit NF4 using `BitsAndBytesConfig`, attach LoRA adapters to attention + MLP modules, and fine-tune with `trl.SFTTrainer` so padding/truncation are handled automatically.\n",
        "\n",
        "### ğŸ”§ BitsAndBytesConfig Deep Dive\n",
        "\n",
        "```python\n",
        "BitsAndBytesConfig(\n",
        "    load_in_4bit=True,              # Enable 4-bit quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",      # Use NF4 (optimal for normal distributions)\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bf16 for speed\n",
        "    bnb_4bit_use_double_quant=True, # Quantize the quantization constants too!\n",
        ")\n",
        "```\n",
        "\n",
        "**Parameter explanations:**\n",
        "- `load_in_4bit`: Quantize weights to 4-bit on load\n",
        "- `bnb_4bit_quant_type=\"nf4\"`: Normal Float 4-bit, optimal for neural network weights\n",
        "- `bnb_4bit_compute_dtype`: Dequantize to bf16 for matrix multiplications\n",
        "- `bnb_4bit_use_double_quant`: Extra 0.4 bits/param savings by quantizing scaling factors\n",
        "\n",
        "### ğŸ”„ Training Flow Diagram\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        QLoRA TRAINING LOOP                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  1. LOAD MODEL IN 4-BIT                                            â”‚\n",
        "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
        "â”‚     â”‚  for each weight matrix W:                              â”‚    â”‚\n",
        "â”‚     â”‚    quantized_W = quantize_nf4(W)  # 4-bit storage      â”‚    â”‚\n",
        "â”‚     â”‚    save scaling_factors           # for dequantization â”‚    â”‚\n",
        "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  2. PREPARE FOR K-BIT TRAINING                                     â”‚\n",
        "â”‚     â””â”€â–º Cast layer norms to fp32 for training stability           â”‚\n",
        "â”‚     â””â”€â–º Enable gradient checkpointing                              â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  3. ATTACH LoRA ADAPTERS                                           â”‚\n",
        "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
        "â”‚     â”‚  Target modules: q,k,v,o_proj + gate,up,down_proj       â”‚    â”‚\n",
        "â”‚     â”‚  Each gets trainable A (rÃ—k) and B (dÃ—r) matrices      â”‚    â”‚\n",
        "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  4. FORWARD PASS (for each batch)                                  â”‚\n",
        "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
        "â”‚     â”‚  W_deq = dequantize(W_4bit) â†’ bf16  # On-the-fly       â”‚    â”‚\n",
        "â”‚     â”‚  output = W_deq @ x + (B @ A @ x) Ã— scale              â”‚    â”‚\n",
        "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  5. BACKWARD PASS                                                  â”‚\n",
        "â”‚     â””â”€â–º Gradients computed ONLY for A and B matrices              â”‚\n",
        "â”‚     â””â”€â–º Base 4-bit weights remain frozen                          â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  6. OPTIMIZER STEP                                                 â”‚\n",
        "â”‚     â””â”€â–º Update only LoRA parameters (~0.1% of total)              â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### âš™ï¸ Key Training Arguments Explained\n",
        "\n",
        "| Argument | Value | Why |\n",
        "|----------|-------|-----|\n",
        "| `bf16=True` | Use bfloat16 | Matches compute dtype, faster training |\n",
        "| `max_grad_norm=0.3` | Gradient clipping | Prevents training instability |\n",
        "| `warmup_ratio=0.03` | Gradual LR increase | Helps with 4-bit training stability |\n",
        "| `packing=True` | Combine short sequences | More efficient GPU utilization |\n",
        "\n",
        "### ğŸ¯ Target Modules for Llama/Mistral\n",
        "\n",
        "```python\n",
        "target_modules=[\n",
        "    \"q_proj\",    # Query projection in attention\n",
        "    \"k_proj\",    # Key projection in attention\n",
        "    \"v_proj\",    # Value projection in attention\n",
        "    \"o_proj\",    # Output projection in attention\n",
        "    \"gate_proj\", # Gating in MLP (SwiGLU)\n",
        "    \"up_proj\",   # Up-projection in MLP\n",
        "    \"down_proj\", # Down-projection in MLP\n",
        "]\n",
        "```\n",
        "\n",
        "Applying LoRA to both attention AND MLP layers gives the best results for instruction-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "if cfg.wandb_project:\n",
        "    os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project\n",
        "else:\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=cfg.lora_r,\n",
        "    lora_alpha=cfg.lora_alpha,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=cfg.lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=cfg.output_dir,\n",
        "    per_device_train_batch_size=cfg.micro_batch_size,\n",
        "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
        "    num_train_epochs=cfg.epochs,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    bf16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    weight_decay=cfg.weight_decay,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=([] if os.environ.get(\"WANDB_DISABLED\") == \"true\" else [\"wandb\"]),\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=splits[\"train\"],\n",
        "    eval_dataset=splits[\"test\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=cfg.cutoff_len,\n",
        "    packing=True,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(cfg.output_dir)\n",
        "tokenizer.save_pretrained(cfg.output_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Model Evaluation: Perplexity\n",
        "\n",
        "**Perplexity** is the standard metric for language models. It measures how well the model predicts the test data:\n",
        "\n",
        "$$\\text{Perplexity} = \\exp\\left(\\frac{1}{N}\\sum_{i=1}^{N} -\\log P(w_i | w_{<i})\\right)$$\n",
        "\n",
        "**Interpretation:**\n",
        "- Lower perplexity = better predictions\n",
        "- Perplexity of 1 = perfect prediction (impossible in practice)\n",
        "- Good domain-specific fine-tuning: 5-15\n",
        "- Base model on specialized domain: 20-100+\n",
        "\n",
        "### âš ï¸ Note on QLoRA Evaluation\n",
        "The model remains in 4-bit during evaluation. For the most accurate perplexity, you could dequantize first, but this requires more memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def compute_perplexity(eval_dataset, max_batches: int = 32) -> float:\n",
        "    model.eval()\n",
        "    ppl_scores = []\n",
        "    loader = DataLoader(eval_dataset[\"text\"], batch_size=1)\n",
        "    for idx, batch in enumerate(loader):\n",
        "        if idx >= max_batches:\n",
        "            break\n",
        "        encoded = tokenizer(batch[0], return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded, labels=encoded[\"input_ids\"])\n",
        "        ppl_scores.append(math.exp(outputs.loss.item()))\n",
        "    return sum(ppl_scores) / len(ppl_scores)\n",
        "\n",
        "\n",
        "perplexity = compute_perplexity(splits[\"test\"], max_batches=32)\n",
        "print(f\"Approximate perplexity: {perplexity:.2f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ—£ï¸ Interactive Testing\n",
        "\n",
        "Test your QLoRA fine-tuned model with the `chat()` function:\n",
        "\n",
        "**Generation parameters:**\n",
        "- `do_sample=True`: Enable sampling (vs greedy decoding)\n",
        "- `top_p=0.9`: Nucleus sampling - only consider tokens in top 90% probability mass\n",
        "- `temperature=0.7`: Controls randomness (lower = more focused, higher = more creative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def chat(prompt: str, system: str | None = None, max_new_tokens: int = 512) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(template, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    output = generated[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "chat(\"Summarize the three most important facts from our dataset.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ”€ Merging QLoRA Adapters\n",
        "\n",
        "After training, you can merge the LoRA adapters into the base model for easier deployment:\n",
        "\n",
        "**Merge process for QLoRA:**\n",
        "```\n",
        "1. Load QLoRA checkpoint (4-bit model + LoRA adapters)\n",
        "2. Dequantize 4-bit weights â†’ FP16\n",
        "3. Merge: W_merged = W_dequant + B Ã— A Ã— (Î±/r)\n",
        "4. Save as standard FP16 model\n",
        "```\n",
        "\n",
        "**Deployment options:**\n",
        "\n",
        "| Option | Pros | Cons |\n",
        "|--------|------|------|\n",
        "| Keep separate (4-bit + LoRA) | Smallest size, swap adapters | Requires bitsandbytes |\n",
        "| Merge to FP16 | Standard format, no dependencies | Larger size (~14 GB for 7B) |\n",
        "| Re-quantize merged model | Small + standard | Slight quality loss |\n",
        "\n",
        "**For production:** Merge and re-quantize with GPTQ/AWQ for the best balance of size and quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "\n",
        "MERGED_DIR = Path(cfg.output_dir) / \"merged\"\n",
        "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "merged_model = AutoPeftModelForCausalLM.from_pretrained(cfg.output_dir, device_map=\"auto\")\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "merged_model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "print(\"Merged checkpoint saved to\", MERGED_DIR)\n",
        "\n",
        "upload_to_hub = False  # flip to True to push\n",
        "if upload_to_hub:\n",
        "    repo_id = input(\"Target HF repo (e.g. username/project-name): \")\n",
        "    merged_model.push_to_hub(repo_id, private=True)\n",
        "    tokenizer.push_to_hub(repo_id, private=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Next Steps & Best Practices\n",
        "\n",
        "### ğŸ“ Workflow Recommendations\n",
        "- Duplicate this notebook per project so you can version-control configs\n",
        "- Replace the evaluation prompts with task-specific checklists (safety, compliance, tone)\n",
        "- Monitor training with W&B for loss curves and gradient norms\n",
        "\n",
        "### ğŸš€ Deployment Options\n",
        "\n",
        "| Framework | QLoRA Support | Best For |\n",
        "|-----------|---------------|----------|\n",
        "| **vLLM** | âœ… Native | High-throughput serving |\n",
        "| **TGI** | âœ… Native | Production APIs |\n",
        "| **llama.cpp** | Merge first | Edge/CPU deployment |\n",
        "| **Ollama** | Merge + GGUF | Local development |\n",
        "\n",
        "### ğŸ”§ Troubleshooting Common Issues\n",
        "\n",
        "| Issue | Solution |\n",
        "|-------|----------|\n",
        "| OOM during training | Reduce `micro_batch_size` or `cutoff_len` |\n",
        "| Loss not decreasing | Increase `learning_rate` or `lora_r` |\n",
        "| Overfitting | Add more `lora_dropout`, reduce epochs |\n",
        "| Poor generation quality | Check prompt template matches training |\n",
        "\n",
        "### ğŸ“š Further Reading\n",
        "- [QLoRA Paper](https://arxiv.org/abs/2305.14314) - Original research\n",
        "- [LoRA Paper](https://arxiv.org/abs/2106.09685) - Foundation technique\n",
        "- [PEFT Documentation](https://huggingface.co/docs/peft) - Implementation details\n",
        ""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}